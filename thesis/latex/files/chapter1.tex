\chapter{Introduction and Background}\label{ch:chapter1}


\section{Introduction} \label{sec:introduction}
The advent of Large Language Models (LLMs) marks a significant advancement in cutting-edge artificial intelligence systems, demonstrating remarkable performance across a wide range of tasks. Nevertheless, deploying a single LLM-based agent for complex, real-world applications exposes several well-known limitations. These include the lack of long-term memory, difficulties in tailoring model behavior to specific sub-tasks, and a restricted capacity to ground outputs in verifiable data sources. Consequently, a growing shift is observed in both research and industry towards multi-agent systems, wherein agents are assembled, each optimized for distinct tasks and characterized by specific configurations (e.g., varying knowledge bases, hyperparameter tuning, training data, resource management, etc.). This evolution, exemplified by recent frameworks such as SALLMA \cite{becattini2025sallma}, allows for the design of more robust, scalable, and adaptable systems that better accommodate the diverse requirements of contemporary use cases.

The paradigm of multi-agent architectures gives rise to what we term a \textbf{cognitive workflow}\footnote{Although the term cognitive workflow has been used across various domains, its application in this context is limited. We argue that it best captures the organization of semantic workflows that require context adaptation and intelligent decision making, a view supported by the recent literature.}: a structured and coordinated process in which intelligent agents interact to manage context-aware decisions, integrate external tools, and adapt to evolving objectives and data. Unlike traditional workflows, cognitive workflows are designed to support runtime reasoning and reconfiguration, making them particularly powerful, while also introducing a considerable degree of complexity.

As the number of agents, tools, data sources, and configurations grows, so too does the difficulty of ensuring Reliability, Maintainability, and Flexibility\footnote{These qualities—Reliability, Maintainability, and Flexibility—are classified as primary software quality characteristics by the \texttt{ISO/IEC 25010} standard.}. Notably, tracing the dependencies, configurations, and interactions of each agent becomes essential for establishing trust in the behavior of the overall system. To tackle these challenges, we propose applying principles from software supply chain management—particularly, the concept of the Software Bill of Materials (SBOM)—to multi-agent AI systems. This contributes to the emerging trend of the AI Bill of Materials (AIBOM), presented in Section~\ref{sec:historical_context}. Furthermore, the Reflection architectural pattern is employed to promote self-awareness and dynamic reconfiguration, establishing a structured method for trust and maintainability in AI-driven workflows. When applied to cognitive workflows, reflection enhances interoperability across agents and facilitates runtime coordination aligned with evolving system goals. 

The remainder of this chapter is organized as follows: Section~\ref{sec:historical_context} reviews the historical background of SBOM and the emerging concept of AIBOM, along with relevant regulatory considerations. Section~\ref{sec:state_of_art} examines the current state of SBOM, its adoption in AI, and the open challenges. Chapter~\ref{ch:chapter2} presents the proposed architectural framework and its implementation. Ultimately, Chapter~\ref{ch:chapter3} demonstrates a relevant use case in the fields of \textit{AI for Network Engineering} (AI4NE) and \textit{Network Engineering for AI} (NE4AI), while Chapter~\ref{ch:chapter4} discusses results, limitations, and future research directions.




\section{Historical Context} \label{sec:historical_context}
Building on SBOM foundations, this section examines the evolution of SBOMs into responsible AI tools amid an expanding regulatory landscape.

\subsection{Software Bill of Materials}

The concept of a Software Bill of Materials (SBOM) has emerged in response to the growing need for traceable and secure software, along with its underlying dependencies. The U.S. National Telecommunications and Information Administration (NTIA) defines the SBOM as a "nested inventory for software, a list of ingredients that make up software components" \cite{NTIA_SBOM}. That is to say, the SBOM acts as the software counterpart of the Bill of Materials (BOM), a well-established concept in the industrial and manufacturing sectors. An SBOM provides a structured inventory of all software libraries, modules, and dependencies that make up a given system. Therefore, this detailed record enables the parties to address critical security concerns, such as identifying which modules rely on a specific library version (e.g., "Which services use library X, version 1.2?") to mitigate vulnerabilities. Although the concept of an SBOM is not new, its adoption has been hampered by challenges related to integration into existing workflows, as well as the lack of standardization \cite{dalia2024sbom}. However, SBOM's popularity has grown significantly following the issuance of an Executive Order \cite{biden2021} by the White House under President Joe Biden in May 2021. This directive, part of a broader initiative to enhance national cybersecurity, mandates that all federal agencies produce SBOMs for their software components. Taking into account the increasing regulatory requirements, it is reasonable to assume that SBOM techniques adoption will rise as more corporations become aware of the associated benefits (See Section~\ref{sec:state_of_art} for statistics on SBOM adoption and usage).



\subsection{Extending SBOM to AI: AIBOM}

In recent years, the number of software applications and open source projects that rely on machine learning (ML) and artificial intelligence (AI) components has exponentially increased. As a result, SBOM techniques have begun to be experimented with within the context of AI systems. The concept of the AI Bill of Materials (AIBOM) is still emerging and has not yet been extensively documented or covered in the literature. Additionally, discussions on the topic have been limited by a restricted group of researchers.


However, this scenario is likely to evolve, as recent developments have significantly impacted the sector: on 13 March 2024, the European Parliament voted in favor of the \textit{AI Act}, which has entered into force on 1 August 2024.
This act is the first comprehensive regulation of Artificial Intelligence within the European Union. It features a four-category classification of AI systems: unacceptable risk (e.g., social scoring systems), which are prohibited, high-risk AI systems, limited-risk AI systems, and minimal-risk AI systems, which remain unregulated. Within the context of this research, a key aspect of the act is that providers of General Purpose AI (GPAI) models, including foundation models like OpenAI's GPT, are mandated to provide technical documentation and instructions for use, comply with the Copyright Directive, and publish a summary of the training data sources. Several points of the regulation stress the importance of dataset quality and relevance, ensuring transparency, and providing information to deployers. Additionally, recital 71 emphasizes that the technical documentation should be updated regularly throughout the lifetime of the system and that "high-risk AI systems should technically allow for the automatic recording of events, by means of logs, over the duration of the lifetime of the system" \cite{eu_ai_act_2024}.


Beyond software security, this focus on dataset quality and transparency highlights the ethical challenges associated with ML systems, particularly in ensuring fairness and reliability, as well as avoiding biased outcomes. For example, a widely cited case study \cite{winters2020software} involved Google in 2015 when software engineer Jacky Alciné drew attention to a major flaw in Google Photos' image recognition algorithm: it had been classifying black people as "gorillas". The error stemmed from multiple factors, most notably the incompleteness of the training data, which lacked sufficient diversity and failed to represent the broader population. Two years later, the press pointed out that the issue had yet to be addressed.


Lastly, while the \textit{AI Act} is the first-ever legal framework on AI, it is worth noting that on 8 December 2020, the United States issued Executive Order 13960 \cite{federalregister2020}, titled \textit{Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government}, through the Department of the Treasury. Although less prescriptive than the \textit{AI Act}, this document emphasizes that federal agencies must design, develop, acquire, and use AI in a manner that fosters public trust. Furthermore, Section 3 designates \textit{Principles for the Use of AI in Government}, highlighting the importance of transparency, accountability, reliability, safety, and resilience as fundamental requirements.

In conclusion, it is evident that the rapidly growing international regulatory landscape highlights the rapid evolution of AI governance and underscores the pressing need for innovative approaches to ensure legal compliance and the dependability of systems.


\subsection{Literature Review}
Although a review of the literature reveals that the term AIBOM is not entirely novel\footnote{To the best of our knowledge, the term AIBOM was first used by Brian Ka Chan in 2017.}, it gained prominence through the work of Australian researchers, who have introduced it as an emerging concept and have been recurrent contributors to this area of study. Notably, Xia et al. \cite{xia2024empirical} highlight the distinction between SBOMs for conventional software and those tailored for AI systems. Meanwhile, in \textit{Trust in Software Supply Chains: Blockchain-Enabled SBOM and the AIBOM Future} \cite{xia2024trust}, Xia et al. propose a blockchain-based architecture that leverages Verifiable Credentials (VCs) and smart contracts to foster trust and overcome the issues associated with SBOM sharing and accountability. These are fundamental steps towards AIBOM as the authors state "the advent of AI and its application in critical infrastructure necessitates a nuanced understanding of AI software components, including their origins and interdependencies". Moreover, the idea of exploiting verifiable credentials to improve the traceability of ML systems is not unique to this work, and is also found in other studies, such as Barclay et al. \cite{barclay2022providing}. This further underscores the importance of establishing trust in data originators.


One of the most comprehensive insights on the topic is provided by Stalnaker et al \cite{stalnaker2024boms}, who conducted interviews with 138 practitioners from five stakeholder groups, including professionals with AI/ML backgrounds, and highlighted key challenges and emerging opportunities in SBOM usage. According to the authors, AIBOMs have the potential to improve the reproducibility of machine learning models and facilitate dataset verification across academic studies. By providing transparency on model training—detailing aspects such as architecture, hyperparameters, and the use of pre-trained components—they help ensure accountability. Additionally, the concept of DataBOM\footnote{Data Bill of Materials}, first introduced by Barclay et al. \cite{Barclay2019}, is explored in its relationship with AIBOM as it can assist developers in detecting whether a model was trained on biased or unethically sourced datasets, as well as in preventing Data Poisoning attacks\footnote{An adversarial attack where malicious data is injected into the training set to manipulate or degrade the model’s behavior.}. Nevertheless, when the interviewed stakeholders were asked whether the two documents should be merged into a single one, less than 10\% agreed on their integration.



Lastly, Lu et al. \cite{lu2023responsible} have studied system-level design patterns to foster the engineering of what they define as \textit{responsible-AI-by-design}. Among the solutions presented, notable measures include the Bill of Materials, Ethical Knowledge, Ethical Sandbox, and Ethical Twin. The latter can act as an operational infrastructure component that facilitates the monitoring of an AI system’s runtime behavior and decision-making through an abstract simulation model that uses real-world data. Additionally, the necessity of an Ethical Sandbox is highlighted as a strategy to separate AI components from non-AI components by running them in an isolated, self-contained environment.




\section{State of the Art} \label{sec:state_of_art}
This section examines the current state of SBOM documentation in AI systems, with a focus on industry trends, key challenges, and existing gaps related to its integration into multi-agent workflows

\subsection{Current Trends in Industry Adoption}
In January 2022, the Linux Foundation released the \textit{SBOM Readiness Report} \cite{sbom_readiness}, surveying 412 organizations worldwide. The report highlights that 98\% of the respondents are concerned about software security, making it the first organizational priority. Additionally, 82\% of organizations are familiar with the concept of SBOM, 90\% have started their SBOM journey, while 76\% are actively addressing SBOM needs. Although the Linux Foundation projects an increase in SBOM usage, significant challenges persist.

A more recent study by Xia et al. \cite{xia2024empirical} underlines that the adoption of SBOM is not as optimistic, identifying three key aspects of SBOM's state of practice. Based on data from 17 interviews and 65 survey responses, the study reveals that at least 83.1\% of surveyed organizations do not receive SBOMs along with third-party open-source or proprietary software, thereby complicating SBOM integration with external components and limiting its uptake among software vendors. 

Moreover, the misalignment between anticipations and reality regarding SBOM readiness is further highlighted by Kloeg et al.'s work \cite{kloeg2024charting}, which states that 69\% of the interviewed stakeholders observed minimal demand from their clients or did not express demand to their suppliers for SBOM. 

An up-to-date, comprehensive analysis of the challenges is provided by Dalia, Di Sorbo, and Canfora \cite{dalia2024sbom} who identified ten challenges hindering SBOM adoption, including the lack of a unique standard. Several tools—including CycloneDX, SPDX, and Syft—are presented in a comparative examination according to different criteria, concluding that the enabling technologies have not yet achieved maturity and full automation.


\subsection{State of SBOM Integration in AI}
Concerning the integration of SBOM tools into AI and ML contexts, Stalnaker et al. \cite{stalnaker2024boms} assert that standards and support for such tools are nearly absent in these specific sectors. Moreover, their research reveals that 85\% of the interviewees with a Machine Learning background were unfamiliar with any tool support for generating AIBOMs, while 90\% were unaware of tooling for DataBOMs. Nevertheless, the study provides valuable insights. Notably, it reports that CycloneDX (starting from version 1.5) has added a Machine Learning Bill of Materials to its specification \cite{cyclonedxMLBOM}. Furthermore, it draws attention to the Dataset Cards \cite{huggingfaceDatasetCards} of Hugging Face\footnote
{Hugging Face is a leading AI platform that serves as a centralized repository and package manager for AI models and datasets.
See: \href{https://huggingface.co}{https://huggingface.co}.} which, according to two interviewed participants, could serve as a form of DataBOM. Dataset Cards are documents that store metadata on datasets, including data source, format, and possible bias. Furthermore, it should be noted that Hugging Face supports Model Cards \cite{huggingfaceModelCards}, which simplify the documentation of model architecture, training and validation datasets, and evaluation metrics. The theoretical foundation for Model Cards was already established by Mitchell et al. \cite{mitchell2019model} in 2019, who introduced this framework as a fundamental step towards responsible democratization of machine learning, aiming to standardize ethical practices and ensure transparent model reporting. Their work proposed a well-defined schema for Model Cards, outlining key fields and emphasizing their complementarity with dataset documentation paradigms, such as \textit{Datasheets for Datasets} \cite{Gebru2021Datasheets}.


\subsection{Unresolved Gaps in Existing Work}\label{sec:gaps}

Although the adoption of SBOM techniques in traditional software engineering is expected to increase, substantial gaps persist in the literature and industry practice regarding their application to AI systems. A significant limitation lies in the absence of a structured methodology for managing \textit{cognitive workflows} in multi-agent systems. Existing research primarily discusses SBOMs in a general ML context, leveraging model cards \cite{mitchell2019model} or DataBOMs \cite{Barclay2019} to address ethical considerations and provide documentation of dataset sources, or blockchain-based methods \cite{xia2024trust, barclay2022providing} to foster accountability. However, these approaches do not address the unique challenges posed by multi-agent architectures, where multiple AI components interact dynamically in distributed environments. Additionally, while the concept of AI Bill of Materials (AIBOM) is emerging, its development has, to date, remained theoretical, with no concrete strategies for its integration into AI agent orchestration or demonstrations of how SBOM data can be effectively harnessed in practice within the system, such as for querying an agent’s dependencies. To the best of our knowledge, only the already cited work by Lu et al.  \cite{lu2023responsible} has focused on system-level design patterns within the lifecycle of a provisioned AI system.

Furthermore, the dynamic nature of these workflows—where agents may self-adapt, reconfigure, or exchange knowledge—presents additional complexity that is not accounted for in traditional frameworks. Active use of SBOM information for governance and maintenance within the system remains unexplored despite the increasing regulatory requirements for regular system monitoring and documentation updates \cite{eu_ai_act_2024, federalregister2020}, as highlighted in Section~\ref{sec:historical_context}.


In summary, two critical gaps persist in the literature. First, there is a \textbf{lack of system-level design patterns} for multi-agent workflows that can accommodate the complex interdependencies and dynamic interactions between autonomous agents. Second, there is an absence of \textbf{comprehensive AI traceability tools} that ensure provenance, accountability, and transparency across the entire system lifecycle, from initial deployment to ongoing adaptation and knowledge exchange.